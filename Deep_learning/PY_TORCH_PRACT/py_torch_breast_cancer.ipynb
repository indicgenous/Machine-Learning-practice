{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fbdjR3cvaox7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "NnZSAtBSarJl",
        "outputId": "5b753b71-4345-48ca-d221-6dca75584f08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0    842302         M        17.99         10.38          122.80     1001.0   \n",
              "1    842517         M        20.57         17.77          132.90     1326.0   \n",
              "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3  84348301         M        11.42         20.38           77.58      386.1   \n",
              "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0  ...          17.33           184.60      2019.0            0.1622   \n",
              "1  ...          23.41           158.80      1956.0            0.1238   \n",
              "2  ...          25.53           152.50      1709.0            0.1444   \n",
              "3  ...          26.50            98.87       567.7            0.2098   \n",
              "4  ...          16.67           152.20      1575.0            0.1374   \n",
              "\n",
              "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0             0.6656           0.7119                0.2654          0.4601   \n",
              "1             0.1866           0.2416                0.1860          0.2750   \n",
              "2             0.4245           0.4504                0.2430          0.3613   \n",
              "3             0.8663           0.6869                0.2575          0.6638   \n",
              "4             0.2050           0.4000                0.1625          0.2364   \n",
              "\n",
              "   fractal_dimension_worst  Unnamed: 32  \n",
              "0                  0.11890          NaN  \n",
              "1                  0.08902          NaN  \n",
              "2                  0.08758          NaN  \n",
              "3                  0.17300          NaN  \n",
              "4                  0.07678          NaN  \n",
              "\n",
              "[5 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54521589-a77b-4d9a-b202-8f80424216fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 33 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54521589-a77b-4d9a-b202-8f80424216fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-54521589-a77b-4d9a-b202-8f80424216fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-54521589-a77b-4d9a-b202-8f80424216fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uVWukDNbH_r",
        "outputId": "3ec88a0e-e7f5-46b8-ea8f-50a9411a9299"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['id','Unnamed: 32'],inplace=True)"
      ],
      "metadata": {
        "id": "9BOdlckzbP7k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji6VagikbaZp",
        "outputId": "f9e05424-eb93-4eea-8094-db2162c911c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train , x_test , y_train , y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.2)"
      ],
      "metadata": {
        "id": "rOTOjjTTbdNq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "ytWyObE3dGy3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5tXXoA1dl9w",
        "outputId": "e1925776-5a25-470b-af48-28aceecb6e6a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.09864863,  0.41772492,  1.36312433, ...,  1.28796877,\n",
              "         0.55789639,  1.9750311 ],\n",
              "       [ 0.49259345,  0.85511311,  0.59038923, ...,  1.39531099,\n",
              "         0.60457229,  2.27024602],\n",
              "       [ 0.04872205,  0.90371179,  0.12220267, ...,  0.99354439,\n",
              "         3.64977239,  3.33083297],\n",
              "       ...,\n",
              "       [-0.81056746,  2.3501013 , -0.85714609, ..., -1.72374724,\n",
              "        -2.14447727, -1.3461368 ],\n",
              "       [-0.27564552, -0.81112761, -0.23895801, ..., -0.60692811,\n",
              "         0.45971605, -0.10076717],\n",
              "       [ 2.5725293 ,  0.14233236,  2.50776402, ...,  1.74800685,\n",
              "         0.01549024, -0.59607221]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "08Xe0ejQdpE7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FE8xOIOd65A",
        "outputId": "11b08ad6-7d14-4a2c-f47b-ead26b75af56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
              "       1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
              "       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tensor = torch.from_numpy(x_train.astype(np.float32))\n",
        "x_test_tensor = torch.from_numpy(x_test.astype(np.float32))\n",
        "y_train_tensor = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test_tensor = torch.from_numpy(y_test.astype(np.float32))"
      ],
      "metadata": {
        "id": "7slXBE3jd9DQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8JjRcGCecE9",
        "outputId": "4abf036a-f5bb-4d1e-a705-924dabaffffe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([455])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loader for batches\n",
        "\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.features = x\n",
        "    self.target = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.features[idx] , self.target[idx]\n",
        "\n",
        "train_dataset = CustomDataset(x_train_tensor,y_train_tensor)\n",
        "test_dataset = CustomDataset(x_test_tensor,y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
        "test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "QwBnpZpV2e2F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MYSimpleNN(nn.Module):\n",
        "  def __init__(self,num_features):\n",
        "    super().__init__()\n",
        "    self.linear =nn.Linear(num_features,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "      out=self.linear(x)\n",
        "      out =self.sigmoid(out)\n",
        "      return out"
      ],
      "metadata": {
        "id": "Mqm8ieUmZU-5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=0.1\n",
        "epochs = 50\n",
        "\n",
        "# loss function binary cross entrophy\n",
        "loss_fun = nn.BCELoss()"
      ],
      "metadata": {
        "id": "19bitTLXef_u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MYSimpleNN(x_train_tensor.shape[1])\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  for batch_features , batch_loaders in train_loader:\n",
        "\n",
        "    y_hat = model(batch_features)\n",
        "    # print(y_hat)\n",
        "\n",
        "    # loss\n",
        "    loss = loss_fun(y_hat,batch_loaders.view(-1,1))\n",
        "    # print(f'epoch: {epoch+1} , loss: {loss.item()}')\n",
        "\n",
        "      # # zero gradient\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    # parameter update\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    # print loss in each epoch\n",
        "    print(f'epoch: {epoch+1} , loss: {loss.item()}')\n",
        "  print('-'*90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSqrjDz561BZ",
        "outputId": "4738d330-169c-47b4-98df-dabe56569dee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 , loss: 0.6964728832244873\n",
            "epoch: 1 , loss: 0.5414026379585266\n",
            "epoch: 1 , loss: 0.5189958810806274\n",
            "epoch: 1 , loss: 0.4915889799594879\n",
            "epoch: 1 , loss: 0.40532511472702026\n",
            "epoch: 1 , loss: 0.3172112703323364\n",
            "epoch: 1 , loss: 0.27322840690612793\n",
            "epoch: 1 , loss: 0.32358282804489136\n",
            "epoch: 1 , loss: 0.3236711323261261\n",
            "epoch: 1 , loss: 0.3287835717201233\n",
            "epoch: 1 , loss: 0.2189786583185196\n",
            "epoch: 1 , loss: 0.228531613945961\n",
            "epoch: 1 , loss: 0.25415974855422974\n",
            "epoch: 1 , loss: 0.19191601872444153\n",
            "epoch: 1 , loss: 0.19729842245578766\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 2 , loss: 0.20509356260299683\n",
            "epoch: 2 , loss: 0.24876701831817627\n",
            "epoch: 2 , loss: 0.22324003279209137\n",
            "epoch: 2 , loss: 0.1622854620218277\n",
            "epoch: 2 , loss: 0.2068163901567459\n",
            "epoch: 2 , loss: 0.1719987690448761\n",
            "epoch: 2 , loss: 0.14451751112937927\n",
            "epoch: 2 , loss: 0.231827512383461\n",
            "epoch: 2 , loss: 0.18949508666992188\n",
            "epoch: 2 , loss: 0.14916545152664185\n",
            "epoch: 2 , loss: 0.20263957977294922\n",
            "epoch: 2 , loss: 0.18523991107940674\n",
            "epoch: 2 , loss: 0.127340167760849\n",
            "epoch: 2 , loss: 0.11050272732973099\n",
            "epoch: 2 , loss: 0.3363041281700134\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 3 , loss: 0.2141125500202179\n",
            "epoch: 3 , loss: 0.1633436679840088\n",
            "epoch: 3 , loss: 0.11703071743249893\n",
            "epoch: 3 , loss: 0.18854105472564697\n",
            "epoch: 3 , loss: 0.14596986770629883\n",
            "epoch: 3 , loss: 0.09709244966506958\n",
            "epoch: 3 , loss: 0.1336212456226349\n",
            "epoch: 3 , loss: 0.17260082066059113\n",
            "epoch: 3 , loss: 0.09579920023679733\n",
            "epoch: 3 , loss: 0.1241140216588974\n",
            "epoch: 3 , loss: 0.25172513723373413\n",
            "epoch: 3 , loss: 0.0634293481707573\n",
            "epoch: 3 , loss: 0.13697969913482666\n",
            "epoch: 3 , loss: 0.10890739411115646\n",
            "epoch: 3 , loss: 0.2656252086162567\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 4 , loss: 0.19283060729503632\n",
            "epoch: 4 , loss: 0.17332375049591064\n",
            "epoch: 4 , loss: 0.1077035591006279\n",
            "epoch: 4 , loss: 0.1036841869354248\n",
            "epoch: 4 , loss: 0.123964324593544\n",
            "epoch: 4 , loss: 0.12718777358531952\n",
            "epoch: 4 , loss: 0.12892450392246246\n",
            "epoch: 4 , loss: 0.16258685290813446\n",
            "epoch: 4 , loss: 0.15759778022766113\n",
            "epoch: 4 , loss: 0.11823564767837524\n",
            "epoch: 4 , loss: 0.07548113167285919\n",
            "epoch: 4 , loss: 0.09258007258176804\n",
            "epoch: 4 , loss: 0.11538369208574295\n",
            "epoch: 4 , loss: 0.08442926406860352\n",
            "epoch: 4 , loss: 0.09295492619276047\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 5 , loss: 0.14004705846309662\n",
            "epoch: 5 , loss: 0.1548442244529724\n",
            "epoch: 5 , loss: 0.156366229057312\n",
            "epoch: 5 , loss: 0.08114863932132721\n",
            "epoch: 5 , loss: 0.1346767693758011\n",
            "epoch: 5 , loss: 0.05976257845759392\n",
            "epoch: 5 , loss: 0.12278138101100922\n",
            "epoch: 5 , loss: 0.08775505423545837\n",
            "epoch: 5 , loss: 0.14280420541763306\n",
            "epoch: 5 , loss: 0.08994637429714203\n",
            "epoch: 5 , loss: 0.0918913185596466\n",
            "epoch: 5 , loss: 0.09741949290037155\n",
            "epoch: 5 , loss: 0.10825763642787933\n",
            "epoch: 5 , loss: 0.0958411693572998\n",
            "epoch: 5 , loss: 0.19400405883789062\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 6 , loss: 0.08259386569261551\n",
            "epoch: 6 , loss: 0.11162058264017105\n",
            "epoch: 6 , loss: 0.11874674260616302\n",
            "epoch: 6 , loss: 0.08503392338752747\n",
            "epoch: 6 , loss: 0.08517230302095413\n",
            "epoch: 6 , loss: 0.09930866211652756\n",
            "epoch: 6 , loss: 0.1613830029964447\n",
            "epoch: 6 , loss: 0.053807999938726425\n",
            "epoch: 6 , loss: 0.13480105996131897\n",
            "epoch: 6 , loss: 0.09691682457923889\n",
            "epoch: 6 , loss: 0.0739980936050415\n",
            "epoch: 6 , loss: 0.1358909159898758\n",
            "epoch: 6 , loss: 0.05298151075839996\n",
            "epoch: 6 , loss: 0.13755764067173004\n",
            "epoch: 6 , loss: 0.21935535967350006\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 7 , loss: 0.06898828595876694\n",
            "epoch: 7 , loss: 0.09262274205684662\n",
            "epoch: 7 , loss: 0.09699416160583496\n",
            "epoch: 7 , loss: 0.09428855031728745\n",
            "epoch: 7 , loss: 0.09612520784139633\n",
            "epoch: 7 , loss: 0.08134005218744278\n",
            "epoch: 7 , loss: 0.1639251708984375\n",
            "epoch: 7 , loss: 0.04273765906691551\n",
            "epoch: 7 , loss: 0.09283751249313354\n",
            "epoch: 7 , loss: 0.08022219687700272\n",
            "epoch: 7 , loss: 0.07380019128322601\n",
            "epoch: 7 , loss: 0.11636675894260406\n",
            "epoch: 7 , loss: 0.16444717347621918\n",
            "epoch: 7 , loss: 0.10948620736598969\n",
            "epoch: 7 , loss: 0.014652513898909092\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 8 , loss: 0.07509399950504303\n",
            "epoch: 8 , loss: 0.06363151222467422\n",
            "epoch: 8 , loss: 0.10344140231609344\n",
            "epoch: 8 , loss: 0.12200883775949478\n",
            "epoch: 8 , loss: 0.07700231671333313\n",
            "epoch: 8 , loss: 0.05211978033185005\n",
            "epoch: 8 , loss: 0.14540472626686096\n",
            "epoch: 8 , loss: 0.10642807930707932\n",
            "epoch: 8 , loss: 0.05233604460954666\n",
            "epoch: 8 , loss: 0.06890536844730377\n",
            "epoch: 8 , loss: 0.16208738088607788\n",
            "epoch: 8 , loss: 0.07241802662611008\n",
            "epoch: 8 , loss: 0.10938619822263718\n",
            "epoch: 8 , loss: 0.08588936924934387\n",
            "epoch: 8 , loss: 0.04635082557797432\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 9 , loss: 0.09760849177837372\n",
            "epoch: 9 , loss: 0.13012497127056122\n",
            "epoch: 9 , loss: 0.1313655972480774\n",
            "epoch: 9 , loss: 0.04223504289984703\n",
            "epoch: 9 , loss: 0.03939191997051239\n",
            "epoch: 9 , loss: 0.11290011554956436\n",
            "epoch: 9 , loss: 0.0789402648806572\n",
            "epoch: 9 , loss: 0.12585636973381042\n",
            "epoch: 9 , loss: 0.09568434953689575\n",
            "epoch: 9 , loss: 0.10156066715717316\n",
            "epoch: 9 , loss: 0.07796166837215424\n",
            "epoch: 9 , loss: 0.06730036437511444\n",
            "epoch: 9 , loss: 0.0347108319401741\n",
            "epoch: 9 , loss: 0.09244248270988464\n",
            "epoch: 9 , loss: 0.0897083729505539\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 10 , loss: 0.045634400099515915\n",
            "epoch: 10 , loss: 0.06198417395353317\n",
            "epoch: 10 , loss: 0.16767571866512299\n",
            "epoch: 10 , loss: 0.08369962871074677\n",
            "epoch: 10 , loss: 0.06873973459005356\n",
            "epoch: 10 , loss: 0.11261013895273209\n",
            "epoch: 10 , loss: 0.09446688741445541\n",
            "epoch: 10 , loss: 0.048817455768585205\n",
            "epoch: 10 , loss: 0.017633164301514626\n",
            "epoch: 10 , loss: 0.1267811357975006\n",
            "epoch: 10 , loss: 0.03457780182361603\n",
            "epoch: 10 , loss: 0.11281553655862808\n",
            "epoch: 10 , loss: 0.11919992417097092\n",
            "epoch: 10 , loss: 0.09896630793809891\n",
            "epoch: 10 , loss: 0.01655391789972782\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 11 , loss: 0.07741080969572067\n",
            "epoch: 11 , loss: 0.0744490921497345\n",
            "epoch: 11 , loss: 0.1487324833869934\n",
            "epoch: 11 , loss: 0.040142957121133804\n",
            "epoch: 11 , loss: 0.12424366921186447\n",
            "epoch: 11 , loss: 0.04529000446200371\n",
            "epoch: 11 , loss: 0.08643480390310287\n",
            "epoch: 11 , loss: 0.07949185371398926\n",
            "epoch: 11 , loss: 0.02783055230975151\n",
            "epoch: 11 , loss: 0.06783385574817657\n",
            "epoch: 11 , loss: 0.10343804210424423\n",
            "epoch: 11 , loss: 0.07135956734418869\n",
            "epoch: 11 , loss: 0.11446312069892883\n",
            "epoch: 11 , loss: 0.08061226457357407\n",
            "epoch: 11 , loss: 0.07789818197488785\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 12 , loss: 0.03796323016285896\n",
            "epoch: 12 , loss: 0.12291889637708664\n",
            "epoch: 12 , loss: 0.1499725580215454\n",
            "epoch: 12 , loss: 0.13922792673110962\n",
            "epoch: 12 , loss: 0.057896871119737625\n",
            "epoch: 12 , loss: 0.05777401849627495\n",
            "epoch: 12 , loss: 0.047766804695129395\n",
            "epoch: 12 , loss: 0.11007460951805115\n",
            "epoch: 12 , loss: 0.03652205690741539\n",
            "epoch: 12 , loss: 0.12178574502468109\n",
            "epoch: 12 , loss: 0.08478625118732452\n",
            "epoch: 12 , loss: 0.0500657744705677\n",
            "epoch: 12 , loss: 0.02957979217171669\n",
            "epoch: 12 , loss: 0.07397916167974472\n",
            "epoch: 12 , loss: 0.0049164616502821445\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 13 , loss: 0.0737738087773323\n",
            "epoch: 13 , loss: 0.057109251618385315\n",
            "epoch: 13 , loss: 0.07939538359642029\n",
            "epoch: 13 , loss: 0.07948193699121475\n",
            "epoch: 13 , loss: 0.050724904984235764\n",
            "epoch: 13 , loss: 0.09584561735391617\n",
            "epoch: 13 , loss: 0.08564705401659012\n",
            "epoch: 13 , loss: 0.05665929988026619\n",
            "epoch: 13 , loss: 0.11832685768604279\n",
            "epoch: 13 , loss: 0.038142602890729904\n",
            "epoch: 13 , loss: 0.16628803312778473\n",
            "epoch: 13 , loss: 0.0821869745850563\n",
            "epoch: 13 , loss: 0.04422972351312637\n",
            "epoch: 13 , loss: 0.06128038465976715\n",
            "epoch: 13 , loss: 0.006090033333748579\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 14 , loss: 0.03653360530734062\n",
            "epoch: 14 , loss: 0.062495943158864975\n",
            "epoch: 14 , loss: 0.17806419730186462\n",
            "epoch: 14 , loss: 0.07515028119087219\n",
            "epoch: 14 , loss: 0.1747744381427765\n",
            "epoch: 14 , loss: 0.02504092827439308\n",
            "epoch: 14 , loss: 0.024839051067829132\n",
            "epoch: 14 , loss: 0.08744819462299347\n",
            "epoch: 14 , loss: 0.045261215418577194\n",
            "epoch: 14 , loss: 0.05097132921218872\n",
            "epoch: 14 , loss: 0.07554453611373901\n",
            "epoch: 14 , loss: 0.06710778176784515\n",
            "epoch: 14 , loss: 0.07664612680673599\n",
            "epoch: 14 , loss: 0.07032393664121628\n",
            "epoch: 14 , loss: 0.05976099520921707\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 15 , loss: 0.029597831889986992\n",
            "epoch: 15 , loss: 0.07739277929067612\n",
            "epoch: 15 , loss: 0.029928261414170265\n",
            "epoch: 15 , loss: 0.09613732248544693\n",
            "epoch: 15 , loss: 0.15825234353542328\n",
            "epoch: 15 , loss: 0.07593031227588654\n",
            "epoch: 15 , loss: 0.06918264925479889\n",
            "epoch: 15 , loss: 0.08692432940006256\n",
            "epoch: 15 , loss: 0.08201077580451965\n",
            "epoch: 15 , loss: 0.07813379913568497\n",
            "epoch: 15 , loss: 0.03300024941563606\n",
            "epoch: 15 , loss: 0.04153662174940109\n",
            "epoch: 15 , loss: 0.0690099373459816\n",
            "epoch: 15 , loss: 0.0573982372879982\n",
            "epoch: 15 , loss: 0.24291761219501495\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 16 , loss: 0.05885571241378784\n",
            "epoch: 16 , loss: 0.08225321769714355\n",
            "epoch: 16 , loss: 0.047466084361076355\n",
            "epoch: 16 , loss: 0.06815754622220993\n",
            "epoch: 16 , loss: 0.01970011182129383\n",
            "epoch: 16 , loss: 0.1317163109779358\n",
            "epoch: 16 , loss: 0.08841491490602493\n",
            "epoch: 16 , loss: 0.04229748994112015\n",
            "epoch: 16 , loss: 0.032685719430446625\n",
            "epoch: 16 , loss: 0.08846966177225113\n",
            "epoch: 16 , loss: 0.08103188127279282\n",
            "epoch: 16 , loss: 0.05171823874115944\n",
            "epoch: 16 , loss: 0.08869102597236633\n",
            "epoch: 16 , loss: 0.08973515033721924\n",
            "epoch: 16 , loss: 0.22047264873981476\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 17 , loss: 0.08143170922994614\n",
            "epoch: 17 , loss: 0.04013596847653389\n",
            "epoch: 17 , loss: 0.10095764696598053\n",
            "epoch: 17 , loss: 0.13176417350769043\n",
            "epoch: 17 , loss: 0.04002634435892105\n",
            "epoch: 17 , loss: 0.11138045787811279\n",
            "epoch: 17 , loss: 0.08220811933279037\n",
            "epoch: 17 , loss: 0.019273679703474045\n",
            "epoch: 17 , loss: 0.051279645413160324\n",
            "epoch: 17 , loss: 0.045395802706480026\n",
            "epoch: 17 , loss: 0.08463390171527863\n",
            "epoch: 17 , loss: 0.058415867388248444\n",
            "epoch: 17 , loss: 0.022643910720944405\n",
            "epoch: 17 , loss: 0.08791941404342651\n",
            "epoch: 17 , loss: 0.19257844984531403\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 18 , loss: 0.03292663022875786\n",
            "epoch: 18 , loss: 0.09397531300783157\n",
            "epoch: 18 , loss: 0.16696210205554962\n",
            "epoch: 18 , loss: 0.053298987448215485\n",
            "epoch: 18 , loss: 0.055356573313474655\n",
            "epoch: 18 , loss: 0.057510338723659515\n",
            "epoch: 18 , loss: 0.053485848009586334\n",
            "epoch: 18 , loss: 0.13236144185066223\n",
            "epoch: 18 , loss: 0.07030920684337616\n",
            "epoch: 18 , loss: 0.03964738920331001\n",
            "epoch: 18 , loss: 0.07648835331201553\n",
            "epoch: 18 , loss: 0.05351584032177925\n",
            "epoch: 18 , loss: 0.04899387061595917\n",
            "epoch: 18 , loss: 0.04175199568271637\n",
            "epoch: 18 , loss: 0.009948995895683765\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 19 , loss: 0.13043902814388275\n",
            "epoch: 19 , loss: 0.05193198472261429\n",
            "epoch: 19 , loss: 0.07360925525426865\n",
            "epoch: 19 , loss: 0.06250271201133728\n",
            "epoch: 19 , loss: 0.041285816580057144\n",
            "epoch: 19 , loss: 0.07431072741746902\n",
            "epoch: 19 , loss: 0.10846744477748871\n",
            "epoch: 19 , loss: 0.041011787950992584\n",
            "epoch: 19 , loss: 0.06931008398532867\n",
            "epoch: 19 , loss: 0.04563445970416069\n",
            "epoch: 19 , loss: 0.07836964726448059\n",
            "epoch: 19 , loss: 0.054752007126808167\n",
            "epoch: 19 , loss: 0.05009514465928078\n",
            "epoch: 19 , loss: 0.06600875407457352\n",
            "epoch: 19 , loss: 0.0650339275598526\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 20 , loss: 0.12878508865833282\n",
            "epoch: 20 , loss: 0.07228165119886398\n",
            "epoch: 20 , loss: 0.13282302021980286\n",
            "epoch: 20 , loss: 0.06474673748016357\n",
            "epoch: 20 , loss: 0.05976920574903488\n",
            "epoch: 20 , loss: 0.051672469824552536\n",
            "epoch: 20 , loss: 0.04905092343688011\n",
            "epoch: 20 , loss: 0.04300704225897789\n",
            "epoch: 20 , loss: 0.036215998232364655\n",
            "epoch: 20 , loss: 0.06188792362809181\n",
            "epoch: 20 , loss: 0.04653802886605263\n",
            "epoch: 20 , loss: 0.08304186910390854\n",
            "epoch: 20 , loss: 0.0366862453520298\n",
            "epoch: 20 , loss: 0.07769402861595154\n",
            "epoch: 20 , loss: 0.00893739890307188\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 21 , loss: 0.07266747206449509\n",
            "epoch: 21 , loss: 0.054027557373046875\n",
            "epoch: 21 , loss: 0.08485136181116104\n",
            "epoch: 21 , loss: 0.05560576170682907\n",
            "epoch: 21 , loss: 0.036151785403490067\n",
            "epoch: 21 , loss: 0.016757387667894363\n",
            "epoch: 21 , loss: 0.08125848323106766\n",
            "epoch: 21 , loss: 0.057608313858509064\n",
            "epoch: 21 , loss: 0.11877793818712234\n",
            "epoch: 21 , loss: 0.0497375912964344\n",
            "epoch: 21 , loss: 0.03198801353573799\n",
            "epoch: 21 , loss: 0.10789825767278671\n",
            "epoch: 21 , loss: 0.03712836652994156\n",
            "epoch: 21 , loss: 0.1124950647354126\n",
            "epoch: 21 , loss: 0.06794265657663345\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 22 , loss: 0.06293085962533951\n",
            "epoch: 22 , loss: 0.06296209990978241\n",
            "epoch: 22 , loss: 0.11684910953044891\n",
            "epoch: 22 , loss: 0.027463167905807495\n",
            "epoch: 22 , loss: 0.06667705625295639\n",
            "epoch: 22 , loss: 0.05674301087856293\n",
            "epoch: 22 , loss: 0.051111944019794464\n",
            "epoch: 22 , loss: 0.128202423453331\n",
            "epoch: 22 , loss: 0.04835287854075432\n",
            "epoch: 22 , loss: 0.05642540007829666\n",
            "epoch: 22 , loss: 0.07260847091674805\n",
            "epoch: 22 , loss: 0.06382427364587784\n",
            "epoch: 22 , loss: 0.037304095923900604\n",
            "epoch: 22 , loss: 0.05286412313580513\n",
            "epoch: 22 , loss: 0.05707210674881935\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 23 , loss: 0.08463205397129059\n",
            "epoch: 23 , loss: 0.053874049335718155\n",
            "epoch: 23 , loss: 0.054428085684776306\n",
            "epoch: 23 , loss: 0.07772356271743774\n",
            "epoch: 23 , loss: 0.05624500662088394\n",
            "epoch: 23 , loss: 0.09573106467723846\n",
            "epoch: 23 , loss: 0.06075543165206909\n",
            "epoch: 23 , loss: 0.05965347960591316\n",
            "epoch: 23 , loss: 0.042715515941381454\n",
            "epoch: 23 , loss: 0.024305695667862892\n",
            "epoch: 23 , loss: 0.18288812041282654\n",
            "epoch: 23 , loss: 0.031200861558318138\n",
            "epoch: 23 , loss: 0.03572206199169159\n",
            "epoch: 23 , loss: 0.040614791214466095\n",
            "epoch: 23 , loss: 0.01731572113931179\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 24 , loss: 0.027037259191274643\n",
            "epoch: 24 , loss: 0.050571929663419724\n",
            "epoch: 24 , loss: 0.07605724781751633\n",
            "epoch: 24 , loss: 0.08259274065494537\n",
            "epoch: 24 , loss: 0.0703805610537529\n",
            "epoch: 24 , loss: 0.07728485763072968\n",
            "epoch: 24 , loss: 0.03803672268986702\n",
            "epoch: 24 , loss: 0.04663357883691788\n",
            "epoch: 24 , loss: 0.0235066469758749\n",
            "epoch: 24 , loss: 0.056428637355566025\n",
            "epoch: 24 , loss: 0.0658324733376503\n",
            "epoch: 24 , loss: 0.12758658826351166\n",
            "epoch: 24 , loss: 0.04728701710700989\n",
            "epoch: 24 , loss: 0.0477677658200264\n",
            "epoch: 24 , loss: 0.25542742013931274\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 25 , loss: 0.05193844437599182\n",
            "epoch: 25 , loss: 0.023114455863833427\n",
            "epoch: 25 , loss: 0.04712555930018425\n",
            "epoch: 25 , loss: 0.050559431314468384\n",
            "epoch: 25 , loss: 0.040810175240039825\n",
            "epoch: 25 , loss: 0.02705918252468109\n",
            "epoch: 25 , loss: 0.08786452561616898\n",
            "epoch: 25 , loss: 0.08254925906658173\n",
            "epoch: 25 , loss: 0.12111655622720718\n",
            "epoch: 25 , loss: 0.076093390583992\n",
            "epoch: 25 , loss: 0.08022300899028778\n",
            "epoch: 25 , loss: 0.0499081015586853\n",
            "epoch: 25 , loss: 0.06441803276538849\n",
            "epoch: 25 , loss: 0.07909467071294785\n",
            "epoch: 25 , loss: 0.010881940834224224\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 26 , loss: 0.04951127991080284\n",
            "epoch: 26 , loss: 0.025488540530204773\n",
            "epoch: 26 , loss: 0.08233197033405304\n",
            "epoch: 26 , loss: 0.059711579233407974\n",
            "epoch: 26 , loss: 0.08647053688764572\n",
            "epoch: 26 , loss: 0.04815969616174698\n",
            "epoch: 26 , loss: 0.10048720240592957\n",
            "epoch: 26 , loss: 0.049337346106767654\n",
            "epoch: 26 , loss: 0.008871454745531082\n",
            "epoch: 26 , loss: 0.12330684065818787\n",
            "epoch: 26 , loss: 0.013975992798805237\n",
            "epoch: 26 , loss: 0.05060805380344391\n",
            "epoch: 26 , loss: 0.041893571615219116\n",
            "epoch: 26 , loss: 0.13135090470314026\n",
            "epoch: 26 , loss: 0.005635461769998074\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 27 , loss: 0.07553261518478394\n",
            "epoch: 27 , loss: 0.08176659792661667\n",
            "epoch: 27 , loss: 0.07852080464363098\n",
            "epoch: 27 , loss: 0.05397668480873108\n",
            "epoch: 27 , loss: 0.060714878141880035\n",
            "epoch: 27 , loss: 0.041043102741241455\n",
            "epoch: 27 , loss: 0.04701986163854599\n",
            "epoch: 27 , loss: 0.034165311604738235\n",
            "epoch: 27 , loss: 0.020853903144598007\n",
            "epoch: 27 , loss: 0.04950309544801712\n",
            "epoch: 27 , loss: 0.021794868633151054\n",
            "epoch: 27 , loss: 0.0400504544377327\n",
            "epoch: 27 , loss: 0.09468243271112442\n",
            "epoch: 27 , loss: 0.16245578229427338\n",
            "epoch: 27 , loss: 0.007387360092252493\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 28 , loss: 0.036632195115089417\n",
            "epoch: 28 , loss: 0.058354683220386505\n",
            "epoch: 28 , loss: 0.06652236729860306\n",
            "epoch: 28 , loss: 0.03869028016924858\n",
            "epoch: 28 , loss: 0.038540810346603394\n",
            "epoch: 28 , loss: 0.039422765374183655\n",
            "epoch: 28 , loss: 0.04390852525830269\n",
            "epoch: 28 , loss: 0.03988001495599747\n",
            "epoch: 28 , loss: 0.1056264340877533\n",
            "epoch: 28 , loss: 0.06008696183562279\n",
            "epoch: 28 , loss: 0.12459221482276917\n",
            "epoch: 28 , loss: 0.026125166565179825\n",
            "epoch: 28 , loss: 0.09147776663303375\n",
            "epoch: 28 , loss: 0.0793035551905632\n",
            "epoch: 28 , loss: 0.025141775608062744\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 29 , loss: 0.014328709803521633\n",
            "epoch: 29 , loss: 0.037701673805713654\n",
            "epoch: 29 , loss: 0.09409201890230179\n",
            "epoch: 29 , loss: 0.09004513919353485\n",
            "epoch: 29 , loss: 0.02495262771844864\n",
            "epoch: 29 , loss: 0.07138662040233612\n",
            "epoch: 29 , loss: 0.08953636139631271\n",
            "epoch: 29 , loss: 0.030844874680042267\n",
            "epoch: 29 , loss: 0.139664426445961\n",
            "epoch: 29 , loss: 0.04593408480286598\n",
            "epoch: 29 , loss: 0.03708348050713539\n",
            "epoch: 29 , loss: 0.03022998385131359\n",
            "epoch: 29 , loss: 0.051366548985242844\n",
            "epoch: 29 , loss: 0.07339007407426834\n",
            "epoch: 29 , loss: 0.0667002722620964\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 30 , loss: 0.0659341886639595\n",
            "epoch: 30 , loss: 0.07080569118261337\n",
            "epoch: 30 , loss: 0.02079329825937748\n",
            "epoch: 30 , loss: 0.0574524812400341\n",
            "epoch: 30 , loss: 0.04278191551566124\n",
            "epoch: 30 , loss: 0.1301436573266983\n",
            "epoch: 30 , loss: 0.03563690930604935\n",
            "epoch: 30 , loss: 0.036572448909282684\n",
            "epoch: 30 , loss: 0.03898018226027489\n",
            "epoch: 30 , loss: 0.08730099350214005\n",
            "epoch: 30 , loss: 0.07528838515281677\n",
            "epoch: 30 , loss: 0.06452835351228714\n",
            "epoch: 30 , loss: 0.04855835810303688\n",
            "epoch: 30 , loss: 0.02852063998579979\n",
            "epoch: 30 , loss: 0.15152809023857117\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 31 , loss: 0.015555521473288536\n",
            "epoch: 31 , loss: 0.017172465100884438\n",
            "epoch: 31 , loss: 0.06158559024333954\n",
            "epoch: 31 , loss: 0.07945050299167633\n",
            "epoch: 31 , loss: 0.16695240139961243\n",
            "epoch: 31 , loss: 0.028316402807831764\n",
            "epoch: 31 , loss: 0.027527863159775734\n",
            "epoch: 31 , loss: 0.057204850018024445\n",
            "epoch: 31 , loss: 0.06447616964578629\n",
            "epoch: 31 , loss: 0.09978825598955154\n",
            "epoch: 31 , loss: 0.07003751397132874\n",
            "epoch: 31 , loss: 0.0643695592880249\n",
            "epoch: 31 , loss: 0.03582805022597313\n",
            "epoch: 31 , loss: 0.02090633288025856\n",
            "epoch: 31 , loss: 0.08237756043672562\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 32 , loss: 0.09099354594945908\n",
            "epoch: 32 , loss: 0.04610469937324524\n",
            "epoch: 32 , loss: 0.030146462842822075\n",
            "epoch: 32 , loss: 0.11305840313434601\n",
            "epoch: 32 , loss: 0.09062524139881134\n",
            "epoch: 32 , loss: 0.037267960608005524\n",
            "epoch: 32 , loss: 0.06601199507713318\n",
            "epoch: 32 , loss: 0.028651762753725052\n",
            "epoch: 32 , loss: 0.07716827839612961\n",
            "epoch: 32 , loss: 0.09484073519706726\n",
            "epoch: 32 , loss: 0.018764248117804527\n",
            "epoch: 32 , loss: 0.013914360664784908\n",
            "epoch: 32 , loss: 0.041359931230545044\n",
            "epoch: 32 , loss: 0.05992782115936279\n",
            "epoch: 32 , loss: 0.05015496537089348\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 33 , loss: 0.012521946802735329\n",
            "epoch: 33 , loss: 0.053923845291137695\n",
            "epoch: 33 , loss: 0.1163594052195549\n",
            "epoch: 33 , loss: 0.0923779085278511\n",
            "epoch: 33 , loss: 0.019246406853199005\n",
            "epoch: 33 , loss: 0.043416157364845276\n",
            "epoch: 33 , loss: 0.1092628687620163\n",
            "epoch: 33 , loss: 0.09155499935150146\n",
            "epoch: 33 , loss: 0.031658835709095\n",
            "epoch: 33 , loss: 0.05760243535041809\n",
            "epoch: 33 , loss: 0.1003507450222969\n",
            "epoch: 33 , loss: 0.019308174028992653\n",
            "epoch: 33 , loss: 0.02758856862783432\n",
            "epoch: 33 , loss: 0.03624533861875534\n",
            "epoch: 33 , loss: 0.007729180622845888\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 34 , loss: 0.048491574823856354\n",
            "epoch: 34 , loss: 0.08189693093299866\n",
            "epoch: 34 , loss: 0.02351316437125206\n",
            "epoch: 34 , loss: 0.009872044436633587\n",
            "epoch: 34 , loss: 0.02785700559616089\n",
            "epoch: 34 , loss: 0.04617461934685707\n",
            "epoch: 34 , loss: 0.00993948895484209\n",
            "epoch: 34 , loss: 0.11226872354745865\n",
            "epoch: 34 , loss: 0.14362411201000214\n",
            "epoch: 34 , loss: 0.04273178428411484\n",
            "epoch: 34 , loss: 0.06357011944055557\n",
            "epoch: 34 , loss: 0.0690154880285263\n",
            "epoch: 34 , loss: 0.09474537521600723\n",
            "epoch: 34 , loss: 0.0142230074852705\n",
            "epoch: 34 , loss: 0.0753508061170578\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 35 , loss: 0.10615596920251846\n",
            "epoch: 35 , loss: 0.0448383167386055\n",
            "epoch: 35 , loss: 0.054562173783779144\n",
            "epoch: 35 , loss: 0.033814385533332825\n",
            "epoch: 35 , loss: 0.03944757208228111\n",
            "epoch: 35 , loss: 0.05498751252889633\n",
            "epoch: 35 , loss: 0.0697961151599884\n",
            "epoch: 35 , loss: 0.10028637200593948\n",
            "epoch: 35 , loss: 0.059003788977861404\n",
            "epoch: 35 , loss: 0.04104280471801758\n",
            "epoch: 35 , loss: 0.08434683829545975\n",
            "epoch: 35 , loss: 0.02956666611135006\n",
            "epoch: 35 , loss: 0.045642461627721786\n",
            "epoch: 35 , loss: 0.032582901418209076\n",
            "epoch: 35 , loss: 0.006705561187118292\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 36 , loss: 0.10351178795099258\n",
            "epoch: 36 , loss: 0.06624879688024521\n",
            "epoch: 36 , loss: 0.021588565781712532\n",
            "epoch: 36 , loss: 0.025071553885936737\n",
            "epoch: 36 , loss: 0.08061756938695908\n",
            "epoch: 36 , loss: 0.028451520949602127\n",
            "epoch: 36 , loss: 0.03212782368063927\n",
            "epoch: 36 , loss: 0.12045327574014664\n",
            "epoch: 36 , loss: 0.07402901351451874\n",
            "epoch: 36 , loss: 0.03680111840367317\n",
            "epoch: 36 , loss: 0.050287459045648575\n",
            "epoch: 36 , loss: 0.04564175382256508\n",
            "epoch: 36 , loss: 0.04625079408288002\n",
            "epoch: 36 , loss: 0.05843958258628845\n",
            "epoch: 36 , loss: 0.01017387118190527\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 37 , loss: 0.02286645770072937\n",
            "epoch: 37 , loss: 0.05513651669025421\n",
            "epoch: 37 , loss: 0.09663563221693039\n",
            "epoch: 37 , loss: 0.0485677532851696\n",
            "epoch: 37 , loss: 0.0562250018119812\n",
            "epoch: 37 , loss: 0.04882975295186043\n",
            "epoch: 37 , loss: 0.07810313999652863\n",
            "epoch: 37 , loss: 0.045806240290403366\n",
            "epoch: 37 , loss: 0.1544247716665268\n",
            "epoch: 37 , loss: 0.016826767474412918\n",
            "epoch: 37 , loss: 0.05256073549389839\n",
            "epoch: 37 , loss: 0.02685532532632351\n",
            "epoch: 37 , loss: 0.04630434140563011\n",
            "epoch: 37 , loss: 0.030322320759296417\n",
            "epoch: 37 , loss: 0.02317442186176777\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 38 , loss: 0.17706477642059326\n",
            "epoch: 38 , loss: 0.04209491237998009\n",
            "epoch: 38 , loss: 0.03077610209584236\n",
            "epoch: 38 , loss: 0.04091867059469223\n",
            "epoch: 38 , loss: 0.01750582829117775\n",
            "epoch: 38 , loss: 0.060125380754470825\n",
            "epoch: 38 , loss: 0.06037217006087303\n",
            "epoch: 38 , loss: 0.041615333408117294\n",
            "epoch: 38 , loss: 0.06446195393800735\n",
            "epoch: 38 , loss: 0.05706581100821495\n",
            "epoch: 38 , loss: 0.011296240612864494\n",
            "epoch: 38 , loss: 0.03838837891817093\n",
            "epoch: 38 , loss: 0.054881684482097626\n",
            "epoch: 38 , loss: 0.08174607157707214\n",
            "epoch: 38 , loss: 0.00634473143145442\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 39 , loss: 0.02996533364057541\n",
            "epoch: 39 , loss: 0.058329928666353226\n",
            "epoch: 39 , loss: 0.034068334847688675\n",
            "epoch: 39 , loss: 0.1313978135585785\n",
            "epoch: 39 , loss: 0.03643778711557388\n",
            "epoch: 39 , loss: 0.03406178206205368\n",
            "epoch: 39 , loss: 0.0289103165268898\n",
            "epoch: 39 , loss: 0.10530002415180206\n",
            "epoch: 39 , loss: 0.03418813273310661\n",
            "epoch: 39 , loss: 0.036008916795253754\n",
            "epoch: 39 , loss: 0.03376166149973869\n",
            "epoch: 39 , loss: 0.04248104244470596\n",
            "epoch: 39 , loss: 0.10754319280385971\n",
            "epoch: 39 , loss: 0.061683252453804016\n",
            "epoch: 39 , loss: 0.003306871745735407\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 40 , loss: 0.08186063170433044\n",
            "epoch: 40 , loss: 0.032203007489442825\n",
            "epoch: 40 , loss: 0.014528955332934856\n",
            "epoch: 40 , loss: 0.03604676201939583\n",
            "epoch: 40 , loss: 0.07719436287879944\n",
            "epoch: 40 , loss: 0.056481484323740005\n",
            "epoch: 40 , loss: 0.04961055517196655\n",
            "epoch: 40 , loss: 0.04043866693973541\n",
            "epoch: 40 , loss: 0.03381036967039108\n",
            "epoch: 40 , loss: 0.0884038582444191\n",
            "epoch: 40 , loss: 0.03743032366037369\n",
            "epoch: 40 , loss: 0.0248851515352726\n",
            "epoch: 40 , loss: 0.1443188488483429\n",
            "epoch: 40 , loss: 0.035101503133773804\n",
            "epoch: 40 , loss: 0.07035937160253525\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 41 , loss: 0.03182091563940048\n",
            "epoch: 41 , loss: 0.034541673958301544\n",
            "epoch: 41 , loss: 0.04578710347414017\n",
            "epoch: 41 , loss: 0.042835403233766556\n",
            "epoch: 41 , loss: 0.0678013488650322\n",
            "epoch: 41 , loss: 0.08957861363887787\n",
            "epoch: 41 , loss: 0.025750121101737022\n",
            "epoch: 41 , loss: 0.04989878833293915\n",
            "epoch: 41 , loss: 0.049385979771614075\n",
            "epoch: 41 , loss: 0.019035067409276962\n",
            "epoch: 41 , loss: 0.04838576540350914\n",
            "epoch: 41 , loss: 0.14593559503555298\n",
            "epoch: 41 , loss: 0.022327527403831482\n",
            "epoch: 41 , loss: 0.08783368766307831\n",
            "epoch: 41 , loss: 0.003400480141863227\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 42 , loss: 0.03170688822865486\n",
            "epoch: 42 , loss: 0.06536306440830231\n",
            "epoch: 42 , loss: 0.12686310708522797\n",
            "epoch: 42 , loss: 0.04916973412036896\n",
            "epoch: 42 , loss: 0.03547950088977814\n",
            "epoch: 42 , loss: 0.0051894411444664\n",
            "epoch: 42 , loss: 0.04956949129700661\n",
            "epoch: 42 , loss: 0.03073955327272415\n",
            "epoch: 42 , loss: 0.11248603463172913\n",
            "epoch: 42 , loss: 0.02569006197154522\n",
            "epoch: 42 , loss: 0.08341586589813232\n",
            "epoch: 42 , loss: 0.05693063884973526\n",
            "epoch: 42 , loss: 0.01712368056178093\n",
            "epoch: 42 , loss: 0.050826188176870346\n",
            "epoch: 42 , loss: 0.07641836255788803\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 43 , loss: 0.13121527433395386\n",
            "epoch: 43 , loss: 0.016232211142778397\n",
            "epoch: 43 , loss: 0.04416951164603233\n",
            "epoch: 43 , loss: 0.021616438403725624\n",
            "epoch: 43 , loss: 0.06313255429267883\n",
            "epoch: 43 , loss: 0.05291862040758133\n",
            "epoch: 43 , loss: 0.026619719341397285\n",
            "epoch: 43 , loss: 0.06712978333234787\n",
            "epoch: 43 , loss: 0.04017636179924011\n",
            "epoch: 43 , loss: 0.13080048561096191\n",
            "epoch: 43 , loss: 0.02248959429562092\n",
            "epoch: 43 , loss: 0.04701116681098938\n",
            "epoch: 43 , loss: 0.05009379982948303\n",
            "epoch: 43 , loss: 0.023225100710988045\n",
            "epoch: 43 , loss: 0.07082159072160721\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 44 , loss: 0.07754897326231003\n",
            "epoch: 44 , loss: 0.04975857213139534\n",
            "epoch: 44 , loss: 0.050877004861831665\n",
            "epoch: 44 , loss: 0.023110976442694664\n",
            "epoch: 44 , loss: 0.032779887318611145\n",
            "epoch: 44 , loss: 0.0801951065659523\n",
            "epoch: 44 , loss: 0.015451260842382908\n",
            "epoch: 44 , loss: 0.021543940529227257\n",
            "epoch: 44 , loss: 0.026757841929793358\n",
            "epoch: 44 , loss: 0.10268045216798782\n",
            "epoch: 44 , loss: 0.042538683861494064\n",
            "epoch: 44 , loss: 0.043618395924568176\n",
            "epoch: 44 , loss: 0.13890066742897034\n",
            "epoch: 44 , loss: 0.03989946469664574\n",
            "epoch: 44 , loss: 0.006057829596102238\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 45 , loss: 0.03541548177599907\n",
            "epoch: 45 , loss: 0.06442870199680328\n",
            "epoch: 45 , loss: 0.08791899681091309\n",
            "epoch: 45 , loss: 0.11102557927370071\n",
            "epoch: 45 , loss: 0.03174302726984024\n",
            "epoch: 45 , loss: 0.0505777932703495\n",
            "epoch: 45 , loss: 0.030317366123199463\n",
            "epoch: 45 , loss: 0.028548678383231163\n",
            "epoch: 45 , loss: 0.03204239904880524\n",
            "epoch: 45 , loss: 0.08078064769506454\n",
            "epoch: 45 , loss: 0.020888756960630417\n",
            "epoch: 45 , loss: 0.03416535258293152\n",
            "epoch: 45 , loss: 0.09306979924440384\n",
            "epoch: 45 , loss: 0.0409863144159317\n",
            "epoch: 45 , loss: 0.0020051843021064997\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 46 , loss: 0.06756443530321121\n",
            "epoch: 46 , loss: 0.020660528913140297\n",
            "epoch: 46 , loss: 0.07958658784627914\n",
            "epoch: 46 , loss: 0.023787297308444977\n",
            "epoch: 46 , loss: 0.04090704023838043\n",
            "epoch: 46 , loss: 0.013038487173616886\n",
            "epoch: 46 , loss: 0.059121400117874146\n",
            "epoch: 46 , loss: 0.024599740281701088\n",
            "epoch: 46 , loss: 0.0579264871776104\n",
            "epoch: 46 , loss: 0.03515976294875145\n",
            "epoch: 46 , loss: 0.09695206582546234\n",
            "epoch: 46 , loss: 0.028094207867980003\n",
            "epoch: 46 , loss: 0.152370885014534\n",
            "epoch: 46 , loss: 0.03757520765066147\n",
            "epoch: 46 , loss: 0.002112478483468294\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 47 , loss: 0.010875269770622253\n",
            "epoch: 47 , loss: 0.09895153343677521\n",
            "epoch: 47 , loss: 0.01655350998044014\n",
            "epoch: 47 , loss: 0.05389563366770744\n",
            "epoch: 47 , loss: 0.08507327735424042\n",
            "epoch: 47 , loss: 0.021886542439460754\n",
            "epoch: 47 , loss: 0.10429698973894119\n",
            "epoch: 47 , loss: 0.012263680808246136\n",
            "epoch: 47 , loss: 0.09343402832746506\n",
            "epoch: 47 , loss: 0.0683911070227623\n",
            "epoch: 47 , loss: 0.03403614088892937\n",
            "epoch: 47 , loss: 0.02854836732149124\n",
            "epoch: 47 , loss: 0.05512819066643715\n",
            "epoch: 47 , loss: 0.04837353155016899\n",
            "epoch: 47 , loss: 0.010381647385656834\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 48 , loss: 0.005704093258827925\n",
            "epoch: 48 , loss: 0.0474480576813221\n",
            "epoch: 48 , loss: 0.07060685753822327\n",
            "epoch: 48 , loss: 0.029216457158327103\n",
            "epoch: 48 , loss: 0.10989023745059967\n",
            "epoch: 48 , loss: 0.05138230323791504\n",
            "epoch: 48 , loss: 0.060899365693330765\n",
            "epoch: 48 , loss: 0.053921058773994446\n",
            "epoch: 48 , loss: 0.020201202481985092\n",
            "epoch: 48 , loss: 0.022090397775173187\n",
            "epoch: 48 , loss: 0.035901203751564026\n",
            "epoch: 48 , loss: 0.058422498404979706\n",
            "epoch: 48 , loss: 0.036794308573007584\n",
            "epoch: 48 , loss: 0.03741026669740677\n",
            "epoch: 48 , loss: 0.4092850983142853\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 49 , loss: 0.09412072598934174\n",
            "epoch: 49 , loss: 0.04320049658417702\n",
            "epoch: 49 , loss: 0.015203764662146568\n",
            "epoch: 49 , loss: 0.028219671919941902\n",
            "epoch: 49 , loss: 0.03988548740744591\n",
            "epoch: 49 , loss: 0.0065485392697155476\n",
            "epoch: 49 , loss: 0.04231208935379982\n",
            "epoch: 49 , loss: 0.14660055935382843\n",
            "epoch: 49 , loss: 0.06629427522420883\n",
            "epoch: 49 , loss: 0.03636921942234039\n",
            "epoch: 49 , loss: 0.1144525557756424\n",
            "epoch: 49 , loss: 0.03885994479060173\n",
            "epoch: 49 , loss: 0.03354273736476898\n",
            "epoch: 49 , loss: 0.00918794609606266\n",
            "epoch: 49 , loss: 0.05477207154035568\n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 50 , loss: 0.027778714895248413\n",
            "epoch: 50 , loss: 0.05403570830821991\n",
            "epoch: 50 , loss: 0.026452580466866493\n",
            "epoch: 50 , loss: 0.020135439932346344\n",
            "epoch: 50 , loss: 0.02007959969341755\n",
            "epoch: 50 , loss: 0.05242931470274925\n",
            "epoch: 50 , loss: 0.08793071657419205\n",
            "epoch: 50 , loss: 0.05793130025267601\n",
            "epoch: 50 , loss: 0.02770376391708851\n",
            "epoch: 50 , loss: 0.039113856852054596\n",
            "epoch: 50 , loss: 0.04940737038850784\n",
            "epoch: 50 , loss: 0.1552458256483078\n",
            "epoch: 50 , loss: 0.04279370978474617\n",
            "epoch: 50 , loss: 0.061198871582746506\n",
            "epoch: 50 , loss: 0.002980027347803116\n",
            "------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for batch_features,batch_loaders in test_loader:\n",
        "    y_hat= model.forward(batch_features)\n",
        "    y_hat =(y_hat>0.5).float()\n",
        "\n",
        "    accuracy = (y_hat==batch_loaders).float().mean()\n",
        "    print(f'accuracy: {accuracy.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTfjOBD58DJl",
        "outputId": "d5463cf5-a74f-414d-b700-95ed87038ee4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.609375\n",
            "accuracy: 0.517578125\n",
            "accuracy: 0.5\n",
            "accuracy: 0.5061728358268738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model.weights)\n"
      ],
      "metadata": {
        "id": "JftXTs7GUMN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# class modelnn(nn.Module):\n",
        "#    def __init__(self,num_features):\n",
        "#     super().__init__()\n",
        "#     self.network =nn.Sequential(\n",
        "#     nn.Linear(num_features,3),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(3,1) ,\n",
        "#     nn.Sigmoid()\n",
        "\n",
        "#     )\n",
        "\n",
        "#    def forward(self,x):\n",
        "#     out = self.network(x)\n",
        "\n",
        "#     return out"
      ],
      "metadata": {
        "id": "0EbZejzyWmYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQiHtjg0ZCYx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}